"""The S3Client class."""
import asyncio
import logging
from pathlib import Path

import aioboto3
import boto3
import tqdm.asyncio
from aiobotocore.session import get_session as aiobotocore_get_session
from boto3.s3.transfer import MB, TransferConfig
from botocore.client import BaseClient
from botocore.config import Config
from botocore.exceptions import ClientError

from snorefox_med.client.s3_client.settings import (
    S3_CONNECT_TIMEOUT,
    S3_READ_TIMEOUT,
    S3_RETRIES_MAX_ATTEMPTS,
    S3_RETRIES_MODE,
)


class S3DownloadError(Exception):
    """Unsuccessful S3 download."""

    def __init__(self: "S3DownloadError", message: str) -> None:
        """Constructor for S3DownloadError."""
        super().__init__(message)


class S3UploadError(Exception):
    """Unsuccessful S3 upload."""

    def __init__(self: "S3UploadError", message: str) -> None:
        """Constructor for S3UploadError."""
        super().__init__(message)


class S3Client:
    """A class for interacting with Amazon S3."""

    def __init__(self: "S3Client") -> None:
        """Initialize an S3Client."""
        botocore_config = Config(
            retries={"max_attempts": S3_RETRIES_MAX_ATTEMPTS, "mode": S3_RETRIES_MODE},
            connect_timeout=S3_CONNECT_TIMEOUT,
            read_timeout=S3_READ_TIMEOUT,
        )
        self._s3 = boto3.client(
            "s3",
            config=botocore_config,
        )
        # create a custom botocore session
        botocore_session = aiobotocore_get_session()
        # set defaults (timeouts and retries) for all async clients
        botocore_session.set_default_client_config(botocore_config)
        # wrap that botocore session in an aioboto3.Session
        self._async_s3_session = aioboto3.Session(botocore_session=botocore_session)
        self._logger = logging.getLogger(self.__class__.__qualname__)

    @staticmethod
    def build_s3_uri(bucket_name: str, object_key: str) -> str:
        """Builds a S3 URI from a bucket name and object key.

        Args:
            bucket_name (str): The name of the S3 bucket.
            object_key (str): The object key.

        Returns:
            str: The S3 URI.
        """
        return f"s3://{bucket_name}/{object_key}"

    @staticmethod
    def parse_s3_uri(uri: str) -> tuple[str, str]:
        """Parses a S3 URI into its bucket name and object key.

        Args:
            uri (str): The S3 URI to parse.

        Raises:
            ValueError: If the given URI does not start with "s3://".

        Returns:
            tuple: A tuple containing the bucket name and object key.
        """
        # Check if the URI starts with "s3://"
        if not uri.startswith("s3://"):
            exception_message = "Given URI is not an S3 URI."
            raise ValueError(exception_message)

        # Split the URI into bucket name and object key
        uri_without_s3_prefix = uri[5:]
        if "/" not in uri_without_s3_prefix:
            return uri_without_s3_prefix, ""

        bucket_name, object_key = uri_without_s3_prefix.split("/", 1)

        return bucket_name, object_key

    def upload_with_uri(
        self: "S3Client",
        local_path: Path,
        s3_prefix_uri: str,
        multipart_threshold: int = 8 * MB,
    ) -> str | list[str]:
        """Uploads a file or a directory to an Amazon S3 bucket.

        This function takes a local file or directory path, and uploads
        its contents to a S3 bucket, preserving the relative directory structure.

        When the ``local_path`` parameter is a directory, the path hierarchy in
        S3 is generated by concatenating the s3_prefix with the relative paths of
        files within the directory. For example, if ``local_path`` is
        "path/my_directory" and s3_prefix_uri is "s3://bucket_name/s3_folder", a file at
        "path/my_directory/sub_folder/file.txt" would be uploaded to
        "s3://bucket_name/s3_folder/my_directory/sub_folder/file.txt" in the S3 bucket.

        **Note: In case of multiple file uploads, the uploads are performed one by one,
        not concurrently.**

        Args:
            local_path (Path): The local file or directory path to be uploaded.
            s3_prefix_uri (str): The prefix S3 URI to be used for the uploaded file(s)
            in the S3 bucket.
            multipart_threshold (int, optional): The size in bytes above which
            the S3 upload should use multipart uploads, default is
            8 * MB (8 * 1024 * 1024 bytes).

        Raises:
            ValueError: If the given ``local_path`` is neither a file
            nor a directory.
            FileNotFoundError: If the given ``local_path`` does not exist.

        Returns:
            The S3 key(s) of the uploaded file(s).
        """
        bucket_name, object_key = self.parse_s3_uri(s3_prefix_uri)
        return self.upload_with_bucket_name_and_key(local_path, object_key, bucket_name, multipart_threshold)

    def upload_with_bucket_name_and_key(
        self: "S3Client",
        local_path: Path,
        s3_prefix: str,
        bucket_name: str,
        multipart_threshold: int = 8 * MB,
    ) -> str | list[str]:
        """Uploads a file or a directory to an Amazon S3 bucket.

        This function takes a local file or directory path, and uploads
        its contents to an S3 bucket, preserving the relative directory structure.

        When the ``local_path`` parameter is a directory, the path hierarchy in
        S3 is generated by concatenating the s3_prefix with the relative paths of
        files within the directory. For example, if ``local_path`` is
        "path/my_directory" and s3_prefix is "s3_folder", a file at
        "path/my_directory/sub_folder/file.txt" would be uploaded to
        "s3_folder/my_directory/sub_folder/file.txt" in the S3 bucket.

        **Note: In case of multiple file uploads, the uploads are
        performed one by one, not concurrently.**

        Args:
            local_path (Path): The local file or directory path to be uploaded.
            s3_prefix (str): The prefix path to be used for the uploaded file(s)
            in the S3 bucket.
            bucket_name (str): The name of the S3 bucket where the file(s) will
            be uploaded.
            multipart_threshold (int, optional): The size in bytes above which
            the S3 upload should use multipart uploads, default is
            8 * MB (8 * 1024 * 1024 bytes).

        Raises:
            ValueError: If the given ``local_path`` is neither a file
            nor a directory.
            FileNotFoundError: If the given ``local_path`` does not exist.

        Returns:
            The S3 key(s) of the uploaded file(s).
        """
        if not local_path.exists():
            exception_message = f"Given `local_path` does not exist ({local_path})."
            raise FileNotFoundError(exception_message)

        if local_path.is_file():
            s3_key = s3_prefix
            self._logger.debug("Uploading %s to %s in bucket %s.", local_path, s3_key, bucket_name)
            self._s3.upload_file(
                str(local_path),
                bucket_name,
                s3_key,
                Config=TransferConfig(multipart_threshold=multipart_threshold),
            )

            return s3_key

        if local_path.is_dir():
            s3_keys = []
            for current_file in local_path.rglob("*"):
                if current_file.is_file():
                    relative_path = current_file.relative_to(local_path)
                    s3_key = str(Path(s3_prefix) / relative_path)
                    self._logger.debug("Uploading %s to %s in bucket %s.", current_file, s3_key, bucket_name)
                    self._s3.upload_file(
                        str(current_file),
                        bucket_name,
                        s3_key,
                        Config=TransferConfig(multipart_threshold=multipart_threshold),
                    )
                    s3_keys.append(s3_key)

            return s3_keys

        exception_message = "Given `local_path` is neither a file nor a directory."
        raise ValueError(exception_message)

    def download_with_uri(
        self: "S3Client",
        s3_uri: str,
        local_path: Path,
        multipart_threshold: int = 8 * MB,
    ) -> Path | list[Path]:
        """Downloads a file or a directory from an Amazon S3 bucket.

        This function takes an S3 URI and downloads
        its contents to a local path, preserving the relative
        directory structure.

        When the ``s3_uri`` parameter is a prefix, the local path
        hierarchy is generated by using the relative paths of files within
        the S3 bucket under that prefix. For example, if ``s3_uri`` is
        "s3://bucket_name/s3_folder/my_directory" and local_path is "path",
        a file at "s3://bucket_name/s3_folder/my_directory/sub_folder/file.txt"
        in the S3 bucket would be downloaded to
        "path/my_directory/sub_folder/file.txt".

        **Note: In case of multiple file downloads, the downloads are
        performed one by one, not concurrently.**

        Args:
            s3_uri (str): The S3 URI for the file/directory to be downloaded.
            local_path (Path): The local directory path where the file(s)
            will be downloaded.
            multipart_threshold (int, optional): The size in bytes above which
            the S3 download should use multipart downloads, default is
            8 * MB (8 * 1024 * 1024 bytes).

        Raises:
            FileNotFoundError: If the object pointed by the ``s3_uri`` does not
            exist in the S3 bucket.

        Returns:
            The local path(s) of the downloaded file(s).
        """
        bucket_name, object_key = self.parse_s3_uri(s3_uri)
        return self.download_with_bucket_name_and_key(object_key, local_path, bucket_name, multipart_threshold)

    def download_with_bucket_name_and_key(
        self: "S3Client",
        s3_path: str,
        local_path: Path,
        bucket_name: str,
        multipart_threshold: int = 8 * MB,
    ) -> Path | list[Path]:
        """Downloads a file or a directory from an Amazon S3 bucket.

        This function takes an S3 file path or prefix and downloads
        its contents to a local path, preserving the relative
        directory structure.

        When the ``s3_path`` parameter is a prefix, the local path
        hierarchy is generated by using the relative paths of files within
        the S3 bucket under that prefix. For example, if ``s3_path`` is
        "s3_folder/my_directory" and local_path is "path", a file at
        "s3_folder/my_directory/sub_folder/file.txt" in the S3 bucket
        would be downloaded to "path/my_directory/sub_folder/file.txt".

        **Note: In case of multiple file downloads, the downloads are
        performed one by one, not concurrently.**

        Args:
            s3_path (str): The S3 file or prefix path to be downloaded.
            local_path (Path): The local directory path where the file(s)
            will be downloaded.
            bucket_name (str): The name of the S3 bucket where the file(s)
            will be downloaded from.
            multipart_threshold (int, optional): The size in bytes above which
            the S3 download should use multipart downloads, default is
            8 * MB (8 * 1024 * 1024 bytes).

        Raises:
            FileNotFoundError: If the object pointed by the ``s3_path`` does not
            exist in the S3 bucket.

        Returns:
            The local path(s) of the downloaded file(s).
        """
        result = self._s3.list_objects_v2(Bucket=bucket_name, Prefix=s3_path)

        if result.get("Contents") is None:
            exception_message = f"Given object key does not exist in the S3 bucket ({s3_path})."
            raise FileNotFoundError(exception_message)

        # Checking if it's a single file by comparing the key of the first object
        # with the s3_path
        if result["Contents"][0]["Key"] == s3_path:
            self._logger.debug("Downloading %s from bucket %s to %s.", s3_path, bucket_name, local_path)
            self._s3.download_file(
                bucket_name,
                s3_path,
                str(local_path),
                Config=TransferConfig(multipart_threshold=multipart_threshold),
            )
            return local_path

        # It's a directory, we need to download all the files in the directory
        local_paths = []
        for item in result["Contents"]:
            s3_key = item["Key"]
            relative_key = Path(s3_key).relative_to(s3_path)
            download_path = local_path / relative_key
            download_path.parent.mkdir(parents=True, exist_ok=True)
            self._logger.debug("Downloading %s from bucket %s to %s.", s3_key, bucket_name, download_path)
            self._s3.download_file(
                bucket_name,
                s3_key,
                str(download_path),
                Config=TransferConfig(multipart_threshold=multipart_threshold),
            )
            local_paths.append(download_path)

        return local_paths

    def delete_with_bucket_name_and_key(
        self: "S3Client",
        s3_path: str,
        bucket_name: str,
    ) -> None:
        """Deletes a file or a directory from an Amazon S3 bucket.

        **Note: In case of multiple file deletions, the deletions are
        performed one by one, not concurrently.**

        Args:
            s3_path (str): The S3 file or prefix path to be deleted.
            bucket_name (str): The name of the S3 bucket where the file(s)
            will be deleted from.

        Raises:
            FileNotFoundError: If the object pointed by the ``s3_path`` does not
            exist in the S3 bucket.
        """
        result = self._s3.list_objects_v2(Bucket=bucket_name, Prefix=s3_path)

        if result.get("Contents") is None:
            exception_message = f"Given object key does not exist in the S3 bucket ({s3_path})."
            raise FileNotFoundError(exception_message)

        # Checking if it's a single file by comparing the key of the first object
        # with the s3_path
        if result["Contents"][0]["Key"] == s3_path:
            self._logger.debug("Deleting %s from bucket %s.", s3_path, bucket_name)
            self._s3.delete_object(
                Bucket=bucket_name,
                Key=s3_path,
            )

        # It's a directory, we need to delete all the files in the directory
        for item in result["Contents"]:
            s3_key = item["Key"]
            self._logger.debug("Deleting %s from bucket %s.", s3_key, bucket_name)
            self._s3.delete_object(
                Bucket=bucket_name,
                Key=s3_path,
            )

    async def _download_with_retry(
        self: "S3Client",
        s3_uri: str,
        local_path: Path,
        max_retries: int,
        async_s3_client: BaseClient,
    ) -> None:
        for attempt in range(1, max_retries + 1):
            try:
                bucket, key = self.parse_s3_uri(s3_uri)
                await async_s3_client.download_file(bucket, key, str(local_path))
                break  # if successful, break the loop
            except TimeoutError:
                if attempt < max_retries:  # Don't log an error on the last retry
                    self._logger.warning("Download failed for %s, retrying...", s3_uri)
                else:
                    raise

    async def _download_multiple_asynchronously(
        self: "S3Client",
        uri_path_list: list[tuple[str, Path]],
        max_retries: int = 3,
    ) -> list[dict[str, int | str | Path | bool | None]]:
        async with self._async_s3_session.client("s3", config=Config(retries={"mode": "standard"})) as async_s3_client:
            tasks = [
                self._download_with_retry(s3_uri, local_path, max_retries, async_s3_client)
                for s3_uri, local_path in uri_path_list
            ]

            results: list[dict[str, int | str | Path | bool | None]] = []

            # Create a progress bar
            pbar = tqdm.asyncio.tqdm(total=len(tasks), desc="Downloading files", unit="file")

            # Iterate over completed tasks
            for idx, task in enumerate(asyncio.as_completed(tasks)):
                s3_uri, local_path = uri_path_list[idx]
                try:
                    await task
                    results.append(
                        {
                            "idx": idx,
                            "s3_uri": s3_uri,
                            "local_path": local_path,
                            "success": True,
                            "error_message": None,
                        },
                    )  # success
                except (ValueError, ClientError, TimeoutError) as e:
                    results.append(
                        {
                            "idx": idx,
                            "s3_uri": s3_uri,
                            "local_path": local_path,
                            "success": False,
                            "error_message": str(e),
                        },
                    )  # failure
                finally:
                    # Update the progress bar
                    pbar.update(1)

            pbar.close()  # Close the progress bar after all tasks are done
            return results

    def download_multiple_asynchronously(
        self: "S3Client",
        uri_path_list: list[tuple[str, Path]],
        max_retries: int = 3,
    ) -> list[dict[str, int | str | Path | bool | None]]:
        """Downloads multiple files from S3 asynchronously.

        Args:
            uri_path_list (list[tuple[str, Path]]): A list of tuples containing
                the S3 URI of the object and the local path where the object should
                be downloaded.
            max_retries (int, optional): The maximum number of retries for each
                download. Defaults to 3.

        Returns:
            A list of result dictionaries containing:
            ``idx``: The index of the corresponding URI in the ``uri_path_list``.
            ``s3_uri``: The S3 URI.
            ``local_path``: The local path.
            ``success``: Flag for whether the download was successful.
            ``error_message``: The error message if the download failed.
        """
        return asyncio.run(self._download_multiple_asynchronously(uri_path_list, max_retries))

    async def _upload_with_retry(
        self: "S3Client",
        s3_uri: str,
        local_path: Path,
        max_retries: int,
        async_s3_client: BaseClient,
    ) -> None:
        for attempt in range(1, max_retries + 1):
            try:
                bucket, key = self.parse_s3_uri(s3_uri)
                with Path(local_path).open("rb") as file:
                    await async_s3_client.upload_fileobj(file, bucket, key)
                break  # if successful, break the loop
            except TimeoutError:
                if attempt < max_retries:  # Don't log an error on the last retry
                    self._logger.warning("Upload failed for %s, retrying...", s3_uri)
                else:
                    raise

    async def _upload_multiple_asynchronously(
        self: "S3Client",
        uri_path_list: list[tuple[str, Path]],
        max_retries: int = 3,
    ) -> list[dict[str, int | str | Path | bool | None]]:
        async with self._async_s3_session.client("s3", config=Config(retries={"mode": "standard"})) as async_s3_client:
            tasks = [
                self._upload_with_retry(s3_uri, local_path, max_retries, async_s3_client)
                for s3_uri, local_path in uri_path_list
            ]

            results: list[dict[str, int | str | Path | bool | None]] = []

            # Create a progress bar
            pbar = tqdm.asyncio.tqdm(total=len(tasks), desc="Uploading files", unit="file")

            # Iterate over completed tasks
            for idx, task in enumerate(asyncio.as_completed(tasks)):
                s3_uri, local_path = uri_path_list[idx]
                try:
                    await task
                    results.append(
                        {
                            "idx": idx,
                            "s3_uri": s3_uri,
                            "local_path": local_path,
                            "success": True,
                            "error_message": None,
                        },
                    )  # success
                except (ValueError, ClientError, TimeoutError) as e:
                    results.append(
                        {
                            "idx": idx,
                            "s3_uri": s3_uri,
                            "local_path": local_path,
                            "success": False,
                            "error_message": str(e),
                        },
                    )  # failure
                finally:
                    # Update the progress bar
                    pbar.update(1)

            pbar.close()  # Close the progress bar after all tasks are done
            return results

    def upload_multiple_asynchronously(
        self: "S3Client",
        uri_path_list: list[tuple[str, Path]],
        max_retries: int = 3,
    ) -> list[dict[str, int | str | Path | bool | None]]:
        """Uploads multiple files from S3 asynchronously.

        Args:
            uri_path_list (list[tuple[str, Path]]): A list of tuples containing
                the S3 URI of the object and the local path from where the object should
                be uploaded.
            max_retries (int, optional): The maximum number of retries for each
                upload. Defaults to 3.

        Returns:
            A list of result dictionaries containing:
            ``idx``: The index of the corresponding URI in the ``uri_path_list``.
            ``s3_uri``: The S3 URI.
            ``local_path``: The local path.
            ``success``: Flag for whether the upload was successful.
            ``error_message``: The error message if the upload failed.
        """
        return asyncio.run(self._upload_multiple_asynchronously(uri_path_list, max_retries))

    def get_download_url(
        self: "S3Client",
        object_key: str,
        bucket_name: str,
        expiration_in_secs: int = 604800,
    ) -> str:
        """Generates a pre-signed URL for downloading an object from an S3 bucket.

        Args:
            object_key: The s3 key (without ``s3://`` prefix and bucket name)
            of the object to be downloaded.
            bucket_name: The name of the S3 bucket where the object is stored.
            expiration_in_secs: The number of seconds until the pre-signed URL expires.
            Default is 7 days.

        Returns:
            The pre-signed URL for downloading the object.
        """
        pre_signed_url: str = self._s3.generate_presigned_url(
            "get_object",
            Params={"Bucket": bucket_name, "Key": object_key},
            ExpiresIn=expiration_in_secs,
        )

        return pre_signed_url

    def copy_between_buckets(
        self: "S3Client",
        input_s3_uri: str,
        output_bucket_name: str,
        output_object_key: str | None = None,
    ) -> None:
        """Copies an object from one S3 bucket to another.

        Args:
            input_s3_uri (str): URI that points to the object that should be copied.
            output_bucket_name (str): The name of the S3 bucket to which the object should be copied.
            output_object_key (str | None, optional): Object key (name) of the copied over object in the new bucket.
                Defaults to the input object key.
        """
        input_bucket_name, input_object_key = self.parse_s3_uri(input_s3_uri)
        copy_source = {
            "Bucket": input_bucket_name,
            "Key": input_object_key,
        }

        if output_object_key is None:
            output_object_key = input_object_key

        self._s3.copy(copy_source, output_bucket_name, output_object_key)
